{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Movies recommendation\n",
    "\n",
    "In the final project, we will scrape the text of hollywood movies released since 2000, and build a recommender system based on the textual content of the movies obtained from Wikipedia.\n",
    "\n",
    "## Phase 1: Gathering data\n",
    "\n",
    "The wikipedia URLs have the common URL format given below (where XXXX in the below URL format represents the year):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/List_of_American_films_of_XXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "import time\n",
    "import pickle #To save the objects that were created using webscraping\n",
    "import pprint\n",
    "from lxml import html\n",
    "import requests\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import urllib\n",
    "import os\n",
    "#import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step-1 \n",
    "Get the URL of each of the movie released since 2000 year from Wikipedia. The following code blocks will get the URLs along with othre details like cast, director, genre and year. The detailes are obtained in 2 pahses. In first phase we get the Movie names, URL and other details for the movies released between 2000 - 2013 years. In the second phase we get the movies details for the years 2014 to 2016. Since the format of the web page is different, we have to do this in 2 phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Get the movies and URLs for the years 2000-2014\n",
    "URL = list()\n",
    "Movie_Name = list()\n",
    "Director = list()\n",
    "Cast = list()\n",
    "Genre = list()\n",
    "year = list()\n",
    "\n",
    "bs = BeautifulSoup(html)\n",
    "for y in list(range(2000,2014)):\n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_American_films_of_\"+str(y)\n",
    "    html = urlopen(url)\n",
    "    time.sleep(3)\n",
    "    bs = BeautifulSoup(html)\n",
    "    for table in bs.find_all('table', {\"class\":\"wikitable\"}):\n",
    "        for row in table.find_all('tr'):\n",
    "            columns = row.find_all('td')\n",
    "            if len(columns) > 4:\n",
    "                Movie_Name.append(columns[0].get_text())\n",
    "                Director.append(columns[1].get_text())\n",
    "                Cast.append(columns[2].get_text())\n",
    "                Genre.append(columns[3].get_text())\n",
    "                year.append(y)\n",
    "                try:\n",
    "                    a = columns[0].find('a',href=True)['href']\n",
    "                    URL.append(\"https://en.wikipedia.org\"+a)\n",
    "                except:\n",
    "                    URL.append(\"NA\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Get the movies details for the years 2014 to 2016\n",
    "URL1 = list()\n",
    "Movie_Name1 = list()\n",
    "Director1 = list()\n",
    "Cast1 = list()\n",
    "Genre1 = list()\n",
    "year1 = list()\n",
    "\n",
    "for y in range(2014,2017):                    \n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_American_films_of_\"+str(y)\n",
    "    try: \n",
    "        html = urlopen(url)\n",
    "    except:\n",
    "        print(\"problem with the following URL...continuining...:\")\n",
    "        print(url)\n",
    "        continue\n",
    "    time.sleep(3)\n",
    "    bs = BeautifulSoup(html)\n",
    "    for table in bs.find_all('table', {\"class\":\"wikitable\"}):\n",
    "        for row in table.find_all('tr'):\n",
    "            columns = row.find_all('td')\n",
    "            if len(columns) > 3: #To make sure that we are accessing the movies tables only\n",
    "                if len(columns) == 6:\n",
    "                    #print(columns[0].get_text())\n",
    "                    Movie_Name1.append(columns[0].get_text())\n",
    "                    Director1.append(columns[1].get_text())\n",
    "                    Cast1.append(columns[2].get_text())\n",
    "                    Genre1.append(columns[3].get_text())\n",
    "                    year1.append(y)\n",
    "                    try:\n",
    "                        a=columns[0].find('a',href=True)['href']\n",
    "                        URL1.append(\"https://en.wikipedia.org\"+a)\n",
    "                    except:\n",
    "                        URL1.append(\"NA\")\n",
    "                        continue\n",
    "                \n",
    "                if len(columns) == 7:\n",
    "                    #print(columns[1].get_text())\n",
    "                    Movie_Name1.append(columns[1].get_text())\n",
    "                    Director1.append(columns[2].get_text())\n",
    "                    Cast1.append(columns[3].get_text())\n",
    "                    Genre1.append(columns[4].get_text())\n",
    "                    year1.append(y)\n",
    "                    \n",
    "                    try:\n",
    "                        a=columns[1].find('a',href=True)['href']\n",
    "                        URL1.append(\"https://en.wikipedia.org\"+a)\n",
    "                    except:\n",
    "                        URL1.append(\"NA\")\n",
    "                        continue\n",
    "\n",
    "                if len(columns) > 7:\n",
    "                    #print(\"col len:{}\".format(len(columns)))\n",
    "                    #print(columns[2].get_text())\n",
    "                    Movie_Name1.append(columns[2].get_text())\n",
    "                    Director1.append(columns[3].get_text())\n",
    "                    Cast1.append(columns[4].get_text())\n",
    "                    Genre1.append(columns[5].get_text())\n",
    "                    year1.append(y)                    \n",
    "                    try:\n",
    "                        a=columns[2].find('a',href=True)['href']\n",
    "                        URL1.append(\"https://en.wikipedia.org\"+a)\n",
    "                    except:\n",
    "                        URL1.append(\"NA\")\n",
    "                        continue\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results\n",
    "We will save the movies details as a CSV file named URL.csv. This helps us to avoid running step-1 again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(Movie_Name+Movie_Name1,URL+URL1,year+year1,Director+Director1,Cast+Cast1,Genre+Genre1)),\\\n",
    "                  columns=[\"Movie\",\"URL\",\"Year\",\"Director\",\"Cast\",\"Genre\"])\n",
    "\n",
    "#Remove the rows which do not have URL information\n",
    "\n",
    "df = df[df[\"URL\"] != \"NA\"]\n",
    "\n",
    "#Write the file\n",
    "df.to_csv(\"URL.csv\",encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the saved file to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>URL</th>\n",
       "      <th>Year</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>102 Dalmatians</td>\n",
       "      <td>https://en.wikipedia.org/wiki/102_Dalmatians</td>\n",
       "      <td>2000</td>\n",
       "      <td>Kevin Lima</td>\n",
       "      <td>Glenn Close, Gérard Depardieu, Alice Evans</td>\n",
       "      <td>Comedy, family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28 Days</td>\n",
       "      <td>https://en.wikipedia.org/wiki/28_Days_(film)</td>\n",
       "      <td>2000</td>\n",
       "      <td>Betty Thomas</td>\n",
       "      <td>Sandra Bullock, Viggo Mortensen</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 Strikes</td>\n",
       "      <td>https://en.wikipedia.org/wiki/3_Strikes_(film)</td>\n",
       "      <td>2000</td>\n",
       "      <td>DJ Pooh</td>\n",
       "      <td>Brian Hooks, N'Bushe Wright</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The 6th Day</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_6th_Day</td>\n",
       "      <td>2000</td>\n",
       "      <td>Roger Spottiswoode</td>\n",
       "      <td>Arnold Schwarzenegger, Robert Duvall</td>\n",
       "      <td>Science fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Across the Line</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Across_the_Line_...</td>\n",
       "      <td>2000</td>\n",
       "      <td>Martin Spottl</td>\n",
       "      <td>Brad Johnson, Adrienne Barbeau, Brian Bloom</td>\n",
       "      <td>Thriller</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Movie                                                URL  Year  \\\n",
       "0    102 Dalmatians       https://en.wikipedia.org/wiki/102_Dalmatians  2000   \n",
       "1           28 Days       https://en.wikipedia.org/wiki/28_Days_(film)  2000   \n",
       "2         3 Strikes     https://en.wikipedia.org/wiki/3_Strikes_(film)  2000   \n",
       "3       The 6th Day          https://en.wikipedia.org/wiki/The_6th_Day  2000   \n",
       "4   Across the Line  https://en.wikipedia.org/wiki/Across_the_Line_...  2000   \n",
       "\n",
       "             Director                                         Cast  \\\n",
       "0          Kevin Lima   Glenn Close, Gérard Depardieu, Alice Evans   \n",
       "1        Betty Thomas              Sandra Bullock, Viggo Mortensen   \n",
       "2             DJ Pooh                  Brian Hooks, N'Bushe Wright   \n",
       "3  Roger Spottiswoode         Arnold Schwarzenegger, Robert Duvall   \n",
       "4       Martin Spottl  Brad Johnson, Adrienne Barbeau, Brian Bloom   \n",
       "\n",
       "             Genre  \n",
       "0   Comedy, family  \n",
       "1            Drama  \n",
       "2           Comedy  \n",
       "3  Science fiction  \n",
       "4         Thriller  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>URL</th>\n",
       "      <th>Year</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4040</th>\n",
       "      <td>Inferno</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Inferno_(2016_film)</td>\n",
       "      <td>2016</td>\n",
       "      <td>Ron Howard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>Friend Request</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Friend_Request</td>\n",
       "      <td>2016</td>\n",
       "      <td>Simon Verhoeven</td>\n",
       "      <td>Alycia Debnam-Carey</td>\n",
       "      <td>Horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>Rogue One: A Star Wars Story (film)</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Rogue_One</td>\n",
       "      <td>2016</td>\n",
       "      <td>Felicity Jones</td>\n",
       "      <td>Diego Luna</td>\n",
       "      <td>Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043</th>\n",
       "      <td>The Founder</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Founder_(film)</td>\n",
       "      <td>2016</td>\n",
       "      <td>John Lee Hancock</td>\n",
       "      <td>Michael Keaton</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4044</th>\n",
       "      <td>Rings</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Rings_(2016_film)</td>\n",
       "      <td>2016</td>\n",
       "      <td>F. Javier Gutiérrez</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Movie  \\\n",
       "4040                              Inferno   \n",
       "4041                       Friend Request   \n",
       "4042  Rogue One: A Star Wars Story (film)   \n",
       "4043                          The Founder   \n",
       "4044                                Rings   \n",
       "\n",
       "                                                    URL  Year  \\\n",
       "4040  https://en.wikipedia.org/wiki/Inferno_(2016_film)  2016   \n",
       "4041       https://en.wikipedia.org/wiki/Friend_Request  2016   \n",
       "4042            https://en.wikipedia.org/wiki/Rogue_One  2016   \n",
       "4043   https://en.wikipedia.org/wiki/The_Founder_(film)  2016   \n",
       "4044    https://en.wikipedia.org/wiki/Rings_(2016_film)  2016   \n",
       "\n",
       "                 Director                 Cast   Genre  \n",
       "4040           Ron Howard                  NaN     NaN  \n",
       "4041      Simon Verhoeven  Alycia Debnam-Carey  Horror  \n",
       "4042       Felicity Jones           Diego Luna  Sci-Fi  \n",
       "4043     John Lee Hancock       Michael Keaton     NaN  \n",
       "4044  F. Javier Gutiérrez                  NaN     NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(4045, 6)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URL = pd.read_csv(\"URL.csv\")\n",
    "\n",
    "display(URL.head())\n",
    "display(URL.tail())\n",
    "\n",
    "URL.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There are 4045 movies that have to be scraped from Wikipedia. Let us do this as batches. Our goal is to scrape the image of the movie (if exists), along with the plot and initial introduction texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions\n",
    "We will code the following functions to obtain the plot information of the movie, along with the release poster image of the movie.\n",
    "\n",
    "**Open_URL(url)** Gets the HTML content, prepares Beautiful Soup object and returns the Beautiful Soup object. The _url_ parameter represents the complete URL of the webpage.\n",
    "\n",
    "**Get_Text(bs,file)** Gets the plot of the movie. In case plot is not present, gets the text data of all the HTML Paragraphs of the input Beautiful Soup object (the _bs_) parameter. The _file_ parameter contains the desired file name, which will be the name of the file that contains the movie's plot. Returns -1 or 0 or 1. If -1, a severe error has occurred, and NO text was saved, 0 represents a successful parsing of the plot's text (and the website contains Plot HTML ID), and 1 represents a successful parsion of the complete text (since the web page does NOT have Plot ID).\n",
    "\n",
    "**Get_Poster(bs, file)** Gets the release poster of the movie and saves the file. Returns 0 if successful else returns -1.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Open_URL(url):\n",
    "    '''\n",
    "    Gets the HTML content, prepares Beautiful Soup object and \n",
    "    returns the \n",
    "    Beautiful Soup object. \n",
    "    The url parameter represents the complete URL of the webpage.\n",
    "    '''\n",
    "    try:\n",
    "         html = urlopen(url)\n",
    "    except:\n",
    "        return -1\n",
    "    try:\n",
    "        bs = BeautifulSoup(html).encode(\"ascii\")\n",
    "        bs.encode(\"utf-8\")\n",
    "        return bs\n",
    "    except:\n",
    "        return -2       \n",
    "\n",
    "def Get_Plot(bs):    \n",
    "    try:\n",
    "        p = bs.find(\"p\")\n",
    "        initial_paragraph = p.getText()\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "    # collect plot in this list\n",
    "    plot = []\n",
    "    \n",
    "    # find the node with id of \"Plot\"\n",
    "    try:\n",
    "        mark = bs.find(id=\"Plot\")\n",
    "        # walk through the siblings of the parent (H2) node \n",
    "        # until we reach the next H2 node\n",
    "        for elt in mark.parent.nextSiblingGenerator():\n",
    "            if elt.name == \"h2\":\n",
    "                break\n",
    "            if hasattr(elt, \"text\"):\n",
    "                plot.append(elt.text)\n",
    "    except:\n",
    "         return -2\n",
    "    \n",
    "    try:\n",
    "        plot=\"\".join(plot)\n",
    "        text = initial_paragraph + plot\n",
    "        return text\n",
    "    except:\n",
    "        return -3    \n",
    "    \n",
    "\n",
    "    \n",
    "def Get_All_Text(bs):\n",
    "    try:\n",
    "        p = bs.find_all(\"p\")\n",
    "        l = list()\n",
    "        for i in p:\n",
    "            l.append(i.getText())\n",
    "        return \" \".join(l)\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def Save_Text_File(text,text_file_name):\n",
    "    #try:\n",
    "    os.chdir(\"./data\")\n",
    "    #except:\n",
    "    #    return -1\n",
    "    \n",
    "    with open(text_file_name, 'w') as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    try:\n",
    "        os.chdir(\"..\")\n",
    "        return 0\n",
    "    except:\n",
    "        return -2\n",
    "\n",
    "def Get_And_Save_Image(bs,image_file_name):    \n",
    "    try: \n",
    "        img=bs.findAll(\"img\",{\"class\":\"thumbborder\"})\n",
    "        img_URL=\"https:\"+img[0]['src']\n",
    "    except:\n",
    "        return -1\n",
    "     \n",
    "    try:\n",
    "        os.chdir(\"./images\")\n",
    "        ignore=urllib.request.urlretrieve(img_URL,image_file_name)\n",
    "        os.chdir(\"..\")\n",
    "        return 0\n",
    "    except:\n",
    "        return -2\n",
    "\n",
    "def Write_Error(url,msg,file):\n",
    "    with open(file,'a') as f:\n",
    "        f.write(\"\\n\"+msg)\n",
    "        f.write(\"\\n\"+url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning the files download...\n"
     ]
    }
   ],
   "source": [
    "tracker = 0\n",
    "term=1\n",
    "start = time.time() # Get start time\n",
    "print(\"Beginning the files download...\")\n",
    "\n",
    "for movie, url, year in zip(list(URL[\"Movie\"]),list(URL[\"URL\"]),list(URL[\"Year\"])):\n",
    "    #print(\"{},{},{}\".format(movie,url,year))\n",
    "    #Open the URL\n",
    "        \n",
    "    bs=Open_URL(url)\n",
    "\n",
    "    if bs == -1:\n",
    "        Write_Error(url,\"Error in opening the URL\",\"error.txt\")\n",
    "        #print(\"Error in opening the URL: {}\".format(url))\n",
    "        continue\n",
    "        \n",
    "\n",
    "    if bs == -2:\n",
    "        Write_Error(url,\"Error in the creation of bs object for the URL\",\"error.txt\")\n",
    "        #print(\"Error in the creation of bs object for the URL: {}\".format(url))\n",
    "        continue\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    #create a name for the files\n",
    "    image_file_name = str(year)+\"_\"+movie.strip()+\".jpg\"\n",
    "    text_file_name =  str(year)+\"_\"+movie.strip()+\".txt\"\n",
    "\n",
    "    text=Get_Plot(bs)\n",
    "    \n",
    "    if text == -1:\n",
    "        Write_Error(url,\"No paragraphs are found\",\"error.txt\")\n",
    "        #print(\"No paragraphs are found\")\n",
    "        #print(url)\n",
    "        continue\n",
    "\n",
    "    if text == -2:\n",
    "        Write_Error(url,\"Warning: No Plot ID found\",\"error.txt\")\n",
    "        #print(\"Warning: No Plot ID found\")\n",
    "        #print(url)\n",
    "        \n",
    "        text = Get_All_Text(bs)\n",
    "        if text == -1:\n",
    "            Write_Error(url,\"No paragraphs are found\",\"error.txt\")\n",
    "            #print(\"No paragraphs are found\")\n",
    "            #print(url)\n",
    "            continue\n",
    "        \n",
    "    if text == -3:\n",
    "        Write_Error(url,\"Error while appending the main plot with the initial paragraph\",\"error.txt\")\n",
    "        #print(\"Error while appending the main plot with the initial paragraph\")\n",
    "        #print(url)\n",
    "        continue\n",
    "    \n",
    "    status = Save_Text_File(text,text_file_name)\n",
    "    \n",
    "    if status == -1:\n",
    "        Write_Error(url,\"Not able to change the directory to ./data\",\"error.txt\")\n",
    "        #print(\"Not able to change the directory to ./data\")\n",
    "        #print(url)\n",
    "        continue\n",
    "    \n",
    "    if status == -2:\n",
    "        Write_Error(url,\"Not able to change the directory to .. (parent directory) from ./data\",\"error.txt\")\n",
    "        #print(\"Not able to change the directory to .. (parent directory) from ./data\")\n",
    "        #print(url)\n",
    "        continue\n",
    "        \n",
    "    #Downloading Image files    \n",
    "    status = Get_And_Save_Image(bs,image_file_name)\n",
    "    \n",
    "    if status == -1:\n",
    "        Write_Error(url,\"Not able to find the image\",\"error.txt\")\n",
    "        #print(\"Not able to find the image\")\n",
    "        #print(url)\n",
    "        continue\n",
    "\n",
    "    if status == -2:\n",
    "        Write_Error(url,\"Not able to save the image\",\"error.txt\")\n",
    "        #print(\"Not able to save the image\")\n",
    "        #print(url)\n",
    "        continue\n",
    "\n",
    "    #Check the status of the webbot    \n",
    "    tracker = tracker + 1\n",
    "    if (tracker % 100 == 0):\n",
    "        print(\"Processed {} URLs\".format(tracker))\n",
    "        end = time.time() # Get end time\n",
    "        elapsed_time = end - start\n",
    "        print(\"Elapsed time to process 100 URLs:{} secs\".format(elapsed_time))\n",
    "        start = time.time() # Get end time\n",
    "        #break\n",
    "\n",
    "    #if term == 1:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get all text from the URL\n",
    "getAllText(bs):\n",
    "    try:\n",
    "        p = bs.find_all(\"p\")\n",
    "        for i in p:\n",
    "             print(i.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000_102 Dalmatians.jpg\n",
      "Error in extracting plot\n",
      "https://en.wikipedia.org/wiki/102_Dalmatians\n",
      "2000_28 Days.jpg\n",
      "2000_3 Strikes.jpg\n",
      "Error in extracting plot\n",
      "https://en.wikipedia.org/wiki/3_Strikes_(film)\n",
      "2000_The 6th Day.jpg\n",
      "Error in extracting plot\n",
      "https://en.wikipedia.org/wiki/The_6th_Day\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-35a094a2d6ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[1;31m#Create a beautiful soup object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tracker = 0\n",
    "term=1\n",
    "start = time.time() # Get start time\n",
    "\n",
    "for movie, url, year in zip(list(URL[\"Movie\"]),list(URL[\"URL\"]),list(URL[\"Year\"])):\n",
    "    #print(\"{},{},{}\".format(movie,url,year))\n",
    "    #Open the URL\n",
    "    \n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except:\n",
    "        print(\"Not able to connect to the following URL:\")\n",
    "        print(url)\n",
    "        continue\n",
    "        \n",
    "    time.sleep(3)\n",
    "    #Create a beautiful soup object\n",
    "    try:\n",
    "        bs = BeautifulSoup(html)\n",
    "    except:\n",
    "        print(\"Error in the creation of beautifulsoup object\")\n",
    "        print(url)\n",
    "        continue\n",
    "\n",
    "    #create a name for the files\n",
    "    image_file_name = str(year)+\"_\"+movie.strip()+\".jpg\"\n",
    "    text_file_name =  str(year)+\"_\"+movie.strip()+\".txt\"\n",
    "    #print(image_file_name)\n",
    "    #Get the text (initial paragraph first):\n",
    "    try:\n",
    "        p = bs.find(\"p\")\n",
    "        initial_paragraph = p.getText()\n",
    "    except:\n",
    "        print(\"Error in getting the first paragraph\")\n",
    "        print(url)\n",
    "        continue\n",
    "    \n",
    "    # collect plot in this list\n",
    "    plot = []\n",
    "    \n",
    "    # find the node with id of \"Plot\"\n",
    "    try:\n",
    "        mark = bs.find(id=\"Plot\")\n",
    "    except:\n",
    "        print(\"Error in getting plot marker\")\n",
    "        print(url)\n",
    "        continue\n",
    "\n",
    "    # walk through the siblings of the parent (H2) node \n",
    "    # until we reach the next H2 node\n",
    "    try:\n",
    "        for elt in mark.parent.nextSiblingGenerator():\n",
    "            if elt.name == \"h2\":\n",
    "                break\n",
    "            if hasattr(elt, \"text\"):\n",
    "                plot.append(elt.text)\n",
    "    except:\n",
    "        print(\"Error in extracting plot\")\n",
    "        print(url)\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        plot=\"\".join(plot)\n",
    "        text = initial_paragraph + plot\n",
    "    except:\n",
    "        print(\"Error in the joining of text\")\n",
    "        print(url)\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        os.chdir(\"./data\")\n",
    "    except:\n",
    "        print(\"Error while changing the directory to data\")\n",
    "        print(url)\n",
    "        continue\n",
    "    \n",
    "    with open(text_file_name, 'w') as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    try:\n",
    "        os.chdir(\"..\")\n",
    "    except:\n",
    "        print(\"Error while changing the directory after saving data\")\n",
    "        print(url)\n",
    "        continue\n",
    "\n",
    "    \n",
    "    \n",
    "    #Downloading Image files\n",
    "    try: \n",
    "        img=bs.findAll(\"img\",{\"class\":\"thumbborder\"})\n",
    "        img_URL=\"https:\"+img[0]['src']\n",
    "    except:\n",
    "        print(\"Error while processing the image file\")\n",
    "        print(img_URL)\n",
    "        print(url)\n",
    "    \n",
    "    try:\n",
    "        os.chdir(\"./images\")\n",
    "        #ignore = urllib.request.urlretrieve(img_URL,os.path.basename(image_file_name))\n",
    "        #urllib.request.urlretrieve(img_URL,os.path.basename(image_file_name))\n",
    "        ignore=urllib.request.urlretrieve(img_URL,image_file_name)\n",
    "    except:\n",
    "        print(\"Error while changing the directory, to save image file\")\n",
    "        print(img_URL)\n",
    "        print(url)\n",
    "        \n",
    "    try:       \n",
    "        os.chdir(\"..\")\n",
    "    except:\n",
    "        print(\"Error while changing the directory, after saving the image file\")\n",
    "        print(img_URL)\n",
    "        print(url)\n",
    "        \n",
    "    tracker = tracker + 1\n",
    "    if (tracker % 10 == 0):\n",
    "        print(\"Processed {} URLs\".format(tracker))\n",
    "        end = time.time() # Get end time\n",
    "        elapsed_time = end - start\n",
    "        print(\"Elapsed time to process 100 URLs:{} secs\".format(elapsed_time))\n",
    "        start = time.time() # Get end time\n",
    "        break\n",
    "\n",
    "    #if term == 1:\n",
    "    #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url='https://en.wikipedia.org/wiki/102_Dalmatians'\n",
    "html = urlopen(url)\n",
    "bs = BeautifulSoup(html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 Dalmatians is a 2000 American family comedy film directed by Kevin Lima in his live-action directorial debut and produced by Edward S. Feldman and Walt Disney Pictures. It is the sequel to the 1996 film 101 Dalmatians and stars Glenn Close reprising her role as Cruella de Vil as she attempts to steal puppies for her \"grandest\" fur coat yet. Close and Tim McInnerny were the only two actors from the first film to return for the sequel, however. The film was nominated for an Academy Award for Best Costume Design, but lost to Gladiator.[2]   After three years in prison, Cruella de Vil has been cured of her desire for fur coats by Dr. Pavlov and is released into the custody of the probation office on the provision that she will be forced to pay the remainder of her fortune (eight million pounds) to all the dog shelters in the borough of Westminster should she repeat her crime. Cruella therefore mends her working relationship with her valet Alonzo and has him lock away all her fur coats. Cruella's probation officer, Chloe Simon, nevertheless suspects her, partly because Chloe is the owner of the now-adult Dipstick (one of the original 15 puppies from the previous film). Dipstick's mate, Dottie, has recently given birth to three puppies: Domino, Little Dipper and Oddball (who lacks spots). To mend her reputation, Cruella buys the Second Chance Dog shelter, owned by Kevin Shepherd, to resolve its financial insolvency that is on the verge of eviction. Meanwhile, Dr. Pavlov discovers that when his therapy's subjects are subjected to loud noises, they revert to their original states but conceals this discovery. When Big Ben rings in her presence, Cruella reverts to her former personality and enlists the help of French furrier Jean-Pierre LePelt to steal 102 Dalmatian puppies for a new fur coat. When Kevin tells Chloe that if Cruella violates her parole, her entire fortune will go to him, since his dog shelter is the only one in the borough of Westminster, Cruella has Kevin framed for the theft of the puppies and invites Chloe to dinner while LePelt steals Dottie and her three puppies. Dipstick hurries back to the apartment and hides in LePelt's truck but is later captured at the train station. Chloe rushes home to save her pets but arrives too late. She is joined by Kevin, who has escaped from prison with help from his dogs and talking parrot, Waddlesworth. Upon finding a ticket for the Orient Express to Paris dropped by LePelt, Kevin and Chloe attempt and fail to stop Cruella and LePelt, but Oddball and Waddlesworth pursue their enemies secretly. In Paris, Kevin and Chloe save some of the captive puppies, but they are seen and locked in the cellar just as the puppies flee. Cruella goes after the puppies alone. Alonzo, when scolded beyond his patience and had enough of being abused, defeats LePelt and frees Kevin and Chloe and they give chase to a wedding cake factory, where the puppies and Kevin's dogs imprison Cruella in an immense cake. She and LePelt are thereupon arrested. Kevin and Chloe are personally awarded the remnants of Cruella's fortune by Alonzo himself and Oddball's coat develops spots. The early working title was 101 Dalmatians Returns. Production began in December 1998 and ended in mid-November 1999. The film was set to be released on June 30, 2000, but was pushed back to November 22, 2000. Oxford Prison was used for the scene as Cruella walked out of prison. 102 Dalmatians was filmed partially in Paris. On November 6, 1999, Disney released the soundtrack to the movie, including pre-eminently, a cover of Paul Anka's \"Puppy Love\" (sung by Myra)[3] and original songs: Mike Himelstein's \"What Can a Bird Do?\" (voiced by Jeff Bennett), \"My Spot in the World\" (sung by Lauren Christy) and \"Cruella De Vil 2000\" (better known as \"Cruella De Vil (102 Dalmatians),\" sung by Camara Kambon and Mark Campbell[4] of Jack Mack and the Heart Attack[citation needed] – a derivation of \"Cruella de Vil\").[5] The film opened at the third position behind M. Night Shyamalan's Unbreakable and Ron Howard's How the Grinch Stole Christmas. The film did well at the box office, earning $66,957,026 in the U.S. and $116,654,745 overseas, bringing its total to $183,611,771 worldwide.[1] After premiering in New Zealand, the film received positive reviews and was described by media as a \"howling success\".[6] In America, the film received generally negative reviews from critics. On Rotten Tomatoes, the film has a 31% \"Rotten\" rating, based on 90 reviews, with the site's consensus reading \"This sequel to the live-action 101 Dalmatians is simply more of the same. Critics say it also drags in parts-- potentially boring children-- and that it's too violent for a G-rated movie.\"[7] On the similar review site Metacritic, the film has a score of 35/100, based on 24 critics.[8] 102 Dalmatians was released on VHS and DVD on April 3, 2001 and re-released on September 16, 2008. A video game based on the film, that was entitled 102 Dalmatians: Puppies to the Rescue, was released in 2000, with Frankie Muniz as the voice of Domino, Molly Marlette as the voice of Oddball and Susanne Blakeslee as the voice of Cruella de Vil. Horace and Jasper also appeared in the game despite not being present in the film.[9]\n"
     ]
    }
   ],
   "source": [
    "p = bs.find_all(\"p\")\n",
    "l = list()\n",
    "for i in p:\n",
    "    l.append(i.getText())\n",
    "    #print(i.getText())\n",
    "print(\" \".join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rings is a 2017 American supernatural psychological horror film directed by F. Javier Gutiérrez, written by David Loucka, Jacob Aaron Estes and Akiva Goldsman and starring Matilda Lutz, Alex Roe, Johnny Galecki, Vincent D'Onofrio, Aimee Teegarden and Bonnie Morgan. It is the third film in The Ring series and takes place thirteen years after The Ring (2002).\n"
     ]
    }
   ],
   "source": [
    "p = bs.find(\"p\")\n",
    "initial_paragraph = p.getText()\n",
    "print(initial_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Rings is a 2017 American supernatural psychological horror film directed by F. Javier Gutiérrez, written by David Loucka, Jacob Aaron Estes and Akiva Goldsman and starring Matilda Lutz, Alex Roe, Johnny Galecki, Vincent D\\'Onofrio, Aimee Teegarden and Bonnie Morgan. It is the third film in The Ring series and takes place thirteen years after The Ring (2002).In 2013, on an airplane bound for Seattle, passenger Carter reveals to Faith that he has watched Samara Morgan’s cursed videotape. Faith tells another passenger Kelly, who reveals she has seen the tape too. She asks Carter if he made a copy and after learning he hasn\\'t, the airplane begins to malfunction as Samara comes for Carter and eventually causes the plane to crash.Two years later, in 2015, college professor Gabriel buys an old VCR once owned by Carter, discovering the videotape inside. Elsewhere, Julia sees her boyfriend Holt off to college, but becomes concerned when he falls out of contact. She is inspired to find him when a panicked girl, Skye, contacts her, asking for Holt’s whereabouts. Julia meets Gabriel and finds a group of people known as \"The Sevens\", who are involved in an experiment involving the cursed video, watching and filming themselves, before passing the task to another person, called a “tail”.Julia recognizes Skye, who takes her to her apartment to have her watch the video, but Holt warns her not to. Julia locks herself in the bathroom as Skye is murdered by Samara, her tail having been late. Holt reveals that he has watched the tape as well and has 12 hours left. Unwilling to let Holt die, Julia watches his copy and when she picks up the phone, she experiences a vision of a door. The phone burns a mark on her hand. Gabriel notices Julia’s copy of the video cannot be copied and is larger than usual. He discovers extra images within the tape and Julia watches the new footage, which features a mysterious woman: she realizes they must cremate Samara\\'s physical remains.Gabriel sends them to the town Sacrament Valley, where Samara was given a burial after the residents of her island refused to accept the remains. He realizes the mark on Julia’s hand is Braille, translates it, and goes to warn them. Julia and Holt find an unmarked tomb, but when they break in, they find it empty. They are caught and taken to a blind man named Burke, who claims Samara’s body was entombed by the local priest but a flood came, leading the priest to bury her in a Potter\\'s field outside town.Heading for the field, Julia and Holt are stopped due to a car crash and learn Gabriel was involved. He tries to warn Julia of his discovery but is fatally electrocuted by a falling utility pole. After experiencing a vision of Samara’s birth mother, Evelyn, Julia and Holt return to town. Julia goes to the church and discovers a hidden chamber beneath the bell tower, finding evidence that Evelyn was imprisoned there whilst pregnant, held in captivity by the priest after being raped before she escaped eight months into the pregnancy.Julia visits Burke and explains her findings. He attacks her, revealing he was not only the priest but Samara’s biological father, and had blinded himself to escape the reach of his daughter’s powers. Julia pushes him down the stairs, temporarily incapacitating him. Holt rushes to Burke\\'s house, where he is knocked unconscious. Julia is drawn to a room in the house where she discovers Samara’s skeleton behind a wall. Burke appears and tries to choke her to death, informing her that she is the twelfth person Samara has sent to her remains; that destroying her remains would unleash an unspeakable evil upon the world, but a swarm of cicadas fly in, summoning Samara through Julia’s phone. Samara removes Burke’s blindness so she can kill him. Holt recovers and rushes to Julia\\'s aid. That night, he and Julia cremate Samara’s corpse and return home.While Julia is in the shower, Holt notices a voicemail from Gabriel, who warns him of the Braille. Holt translates it, discovering it means “rebirth”. In the bathroom, Julia peels away the skin where the mark was, revealing gray skin underneath. She begins to cough up black hair, from which a cicada is born. Holt unsuccessfully tries to disconnect the computer as Julia’s copy of the video is suddenly sent to everyone on her contact list and begins going viral. Julia gazes into the mirror: her reflection is that of Samara\\'s.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# collect plot in this list\n",
    "plot = []\n",
    "\n",
    "# find the node with id of \"Plot\"\n",
    "mark = bs.find(id=\"Plot\")\n",
    "\n",
    "# walk through the siblings of the parent (H2) node \n",
    "# until we reach the next H2 node\n",
    "for elt in mark.parent.nextSiblingGenerator():\n",
    "    if elt.name == \"h2\":\n",
    "        break\n",
    "    if hasattr(elt, \"text\"):\n",
    "        plot.append(elt.text)\n",
    "\n",
    "# enjoy\n",
    "plot=\"\".join(plot)\n",
    "text = initial_paragraph + plot\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "//upload.wikimedia.org/wikipedia/en/thumb/f/fe/102_dalmatians_poster.jpg/220px-102_dalmatians_poster.jpg\n"
     ]
    }
   ],
   "source": [
    "#https://en.wikipedia.org/wiki/101_Dalmatians_(1996_film)\n",
    "img=bs.findAll(\"img\",{\"class\":\"thumbborder\"})\n",
    "print(img[0]['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img=bs.findAll(\"\",{\"class\":\"image\"})\n",
    "for i in img:\n",
    "    img_URL = \"https://en.wikipedia.org\"+i['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://upload.wikimedia.org/wikipedia/en/thumb/f/fe/102_dalmatians_poster.jpg/220px-102_dalmatians_poster.jpg\n"
     ]
    }
   ],
   "source": [
    "img=bs.findAll(\"img\",{\"class\":\"thumbborder\"})\n",
    "img_URL=\"https:\"+img[0]['src']\n",
    "print(img_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "os.chdir(\"./images\")\n",
    "#ignore = urllib.request.urlretrieve(img_URL,os.path.basename(\"./images/\"+img_URL))\n",
    "ignore = urllib.request.urlretrieve(img_URL,os.path.basename(\"img_URL.jpg\"))\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir(\"./images\")\n",
    "#ignore = urllib.request.urlretrieve(img_URL,os.path.basename(\"./images/\"+img_URL))\n",
    "ignore = urllib.request.urlretrieve(img_URL,os.path.basename(\"img_URL.jpg\"))\n",
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
